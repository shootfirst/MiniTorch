# MiniTorchç¬”è®°

## Module0

### Task 0.1: Operators

è¯¥ä»»åŠ¡æ˜¯å®Œæˆminitorch/operators.pyæ–‡ä»¶ä¸­çš„æ•°å­¦å‡½æ•°ï¼Œå”¯ä¸€éœ€è¦æ³¨æ„çš„æ˜¯logå‡½æ•°çš„å®ç°ï¼Œä¸ºäº†é˜²æ­¢å‘ä¸‹æº¢å‡ºï¼Œè¾“å…¥çš„å‚æ•°å˜ä¸º0ï¼Œå¯¹xåŠ ä¸€ä¸ªæå°çš„æµ®ç‚¹å€¼1e-6

ã€ã€ã€

EPS = 1e-6

def log(x):
    return math.log(x + EPS)
    
ã€ã€ã€

### Task 0.2: Testing and Debugging

ç»™ä»»åŠ¡1å†™çš„å‡½æ•°å†™å•å…ƒæµ‹è¯•ï¼Œæ²¡æœ‰ç‰¹åˆ«æ³¨æ„çš„ç»†èŠ‚ï¼Œå¯¹å‡½æ•°ç‰¹æ€§è¿›è¡Œæµ‹è¯•å³å¯

### Task 0.3: Functional Python

å®Œæˆmapã€zipå’Œreduceä¸‰ä¸ªåŸºæœ¬å‡½æ•°

ã€ã€ã€

def map(func):
     return lambda list: [func(x) for x in list]
     
ã€ã€ã€

ã€ã€ã€

def zipWith(func):
    return lambda list1, list2: [func(x, y) for x, y in zip(list1, list2)]
    
ã€ã€ã€

map å‡½æ•°ç”¨äºå¯¹åºåˆ—ä¸­çš„æ¯ä¸ªé¡¹ç›®åº”ç”¨ä¸€ä¸ªå‡½æ•°
ã€ã€ã€

def reduce(func, start):
    def _reduce(func, list, start):
        iterator = iter(list)
        for i in iterator:
            start = func(start, i)
        return start
    return lambda list: _reduce(func, list, start)
    
ã€ã€ã€

é€šè¿‡ä¸‰ä¸ªå‡½æ•°å®ç°negListã€addListsã€sumã€prod

ã€ã€ã€

def negList(list):
    return map(neg)(list)
    
ã€ã€ã€

ã€ã€ã€

def addLists(list1, list2):
    return zipWith(add)(list1, list2)
    
ã€ã€ã€

ã€ã€ã€

def sum(list):
    return reduce(add, 0)(list)
    
ã€ã€ã€

ã€ã€ã€

def prod(list):
    return reduce(mul, 1)(list)
    
ã€ã€ã€

### Task 0.4: Modules

å®ç°moduleç±»ã€‚åœ¨ PyTorch ä¸­ï¼Œtorch.nn.Module æ˜¯æ„å»ºç¥ç»ç½‘ç»œçš„åŸºæœ¬å•å…ƒç±»ã€‚æ‰€æœ‰çš„æ¨¡å‹ã€å±‚ï¼Œä»¥åŠè‡ªå®šä¹‰æ¨¡å—ï¼Œéƒ½éœ€è¦ç»§æ‰¿è‡ª torch.nn.Moduleã€‚å®ƒæä¾›äº†ä¸€ä¸ªæ˜“äºä½¿ç”¨å’Œé«˜åº¦çµæ´»çš„æ¡†æ¶æ¥åˆ›å»ºå¤æ‚çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚
ä¸»è¦å‡½æ•°ï¼š
__init__: æ„é€ å‡½æ•°ï¼Œåœ¨è¿™é‡Œå®šä¹‰å±‚å’Œå‚æ•°ã€‚
forward: å®šä¹‰å‰å‘ä¼ æ’­é€»è¾‘ã€‚
parameters(): è¿”å›æ¨¡å‹æ‰€æœ‰å‚æ•°çš„è¿­ä»£å™¨ï¼Œé€šå¸¸ç”¨äºä¼˜åŒ–å™¨ã€‚
to(device): å°†æ¨¡å—åŠå‚æ•°ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆå¦‚ GPUï¼‰ã€‚
eval(), train(): è®¾ç½®æ¨¡å—ä¸ºè¯„ä¼°æ¨¡å¼æˆ–è®­ç»ƒæ¨¡å¼ã€‚

å®ç°trainã€evalã€named_parameterså’Œparametersã€‚ä¸»è¦æ˜¯named_parametersçš„å®ç°

ã€ã€ã€

    def named_parameters(self) -> Sequence[Tuple[str, Parameter]]:
        
        def _named_parameters(module, prefix=""):
            for name, param in module._parameters.items():
                yield (prefix + name, param)
            for name, param in module._modules.items():
                yield from _named_parameters(param, prefix + name + ".")

        return list(_named_parameters(self))
        
ã€ã€ã€


## Module1

### Task 1.1: Numerical Derivatives

å®ç°æ•°å€¼å¾®åˆ†

ã€ã€ã€
def central_difference(f: Any, *vals: Any, arg: int = 0, epsilon: float = 1e-6) -> Any:
    vals1 = [v for v in vals]
    vals1[arg] += epsilon
    return (f(*vals1) - f(*vals)) / epsilon
ã€ã€ã€

### Task 1.2: Scalars

å®ŒæˆscalarFunctionä¸­å„ä¸ªå­ç±»Scalarå‡½æ•°çš„forwardå‰å‘ä¼ æ’­å‡½æ•°ï¼Œå…³é”®ç‚¹æ˜¯éœ€è¦å­˜å‚¨å…³é”®å˜é‡æ¥ç»™åå‘ä¼ æ’­å»ä½¿ç”¨

AddåŠ æ³•å‡½æ•°ï¼Œå¯¹åŠ æ³•æ±‚å¯¼ä¸º1ï¼Œæ‰€ä»¥æ— éœ€è®°å½•

ã€ã€ã€
    def forward(ctx: Context, a: float, b: float) -> float:
        return a + b
ã€ã€ã€

Logå‡½æ•°ï¼Œéœ€è¦å­˜å‚¨ä¼ å…¥ç›¸ä¹˜çš„ç³»æ•°ï¼Œä¾›åå‘ä¼ æ’­ä½¿ç”¨

ã€ã€ã€
    def forward(ctx: Context, a: float) -> float:
        ctx.save_for_backward(a)
        return operators.log(a)
ã€ã€ã€

Mulä¹˜æ³•å‡½æ•°ï¼Œæ±‚å¯¼ä¸ºä¹˜æ³•çš„ç³»æ•°ï¼Œæ‰€ä»¥éœ€è¦å­˜å‚¨ç›¸ä¹˜çš„ä¸¤ä¸ªæ•°

ã€ã€ã€
    def forward(ctx: Context, a: float, b: float) -> float:
        ctx.save_for_backward(a,b)
        return a * b
ã€ã€ã€

Negå‡½æ•°ï¼Œæ±‚å¯¼åŠ ä¸Šè´Ÿå·å³å¯

ã€ã€ã€
    def forward(ctx: Context, a: float) -> float:
        return operators.neg(a)
ã€ã€ã€

Sigmoidã€Reluã€ExpåŒç†

### Task 1.3: Chain Rule

å®ç°é“¾å¼æ³•åˆ™chain_ruleå‡½æ•°ã€‚é¦–å…ˆæˆ‘ä»¬éœ€è¦ç†è§£å‡ ä¸ªç±»ï¼šScalarHistoryã€Scalarã€ScalarFunctionã€Context

- Scalarï¼šç”¨äºè‡ªåŠ¨å¾®åˆ†çš„æ ‡é‡å€¼ã€‚å°½å¯èƒ½åœ°è¡¨ç°ä¸ºæ ‡å‡†çš„Pythonæ•°å­—ï¼ŒåŒæ—¶è·Ÿè¸ªå®ƒçš„ç›¸å…³å‡½æ•°æ“ä½œï¼Œåªèƒ½é€šè¿‡ScalarFunctionæ“ä½œã€‚

- ScalarHistoryï¼šå­˜å‚¨å¯¹å½“å‰Scalaræ“ä½œçš„å†å²ï¼ŒåŒ…å«ScalarFunctionå’ŒContext

- ScalarFunctionï¼šç”¨äºæ“ä½œä¸Šé¢çš„Scalarå˜é‡çš„æ•°å­¦å‡½æ•°ï¼Œç”¨äºè¢«ä¸åŒæ•°å­¦å‡½æ•°ç»§æ‰¿

- Contextï¼šå­˜å‚¨å‰å‘ä¼ æ’­éœ€è¦è®°å½•çš„å€¼ï¼Œå¦‚Task1.2ä¸­æåˆ°

ã€ã€ã€
    def chain_rule(self, d_output: Any) -> Iterable[Tuple[Variable, Any]]:
        h = self.history
        assert h is not None
        assert h.last_fn is not None
        assert h.ctx is not None

        x = h.last_fn._backward(h.ctx, d_output)
        return list(zip(h.inputs, x))
ã€ã€ã€


### Task 1.4: Backpropagation

1ã€é¦–å…ˆéœ€è¦å®ç°æ‰€æœ‰çš„åå‘ä¼ æ’­å‡½æ•°ï¼ˆ1.2å®ç°äº†æ­£å‘ä¼ æ’­ï¼‰ï¼Œæ ¸å¿ƒæ˜¯ä½¿ç”¨æ­£å‘ä¼ æ’­æ—¶å­˜å‚¨çš„ä¸Šä¸‹æ–‡

AddåŠ æ³•å‡½æ•°ï¼Œç”±äºåŠ æ³•å¯¼æ•°ä¸º1ï¼Œç›´æ¥è¿”å›å½“å‰çš„å…¥å‚ï¼Œç”±äºåŠ æ³•æœ‰ä¸¤ä¸ªè¾“å…¥å‚æ•°ï¼Œæ‰€ä»¥éœ€è¦è¿”å›ä¸¤ä»½

ã€ã€ã€
    def backward(ctx: Context, d_output: float) -> Tuple[float, ...]:
        return d_output, d_output
ã€ã€ã€

Logå‡½æ•°ï¼Œéœ€è¦ä½¿ç”¨æ­£å‘ä¼ æ’­å­˜å‚¨çš„ç³»æ•°

ã€ã€ã€
    def backward(ctx: Context, d_output: float) -> float:
        (a,) = ctx.saved_values
        return operators.log_back(a, d_output)
ã€ã€ã€

Mulä¹˜æ³•å‡½æ•°ï¼Œä¹˜æ³•å¯¼æ•°ä¸ºå…¶ç›¸ä¹˜çš„ç³»æ•°

ã€ã€ã€
    def backward(ctx: Context, d_output: float) -> Tuple[float, float]:
        a, b = ctx.saved_values
        return b * d_output, a * d_output
ã€ã€ã€

åé¢å‡½æ•°éƒ½æ˜¯ç±»ä¼¼æ€è·¯ï¼Œä¸è¿‡æœ‰ä¸¤ä¸ªå€¼å¾—æ³¨æ„ï¼ŒLTå’ŒEQå¯¼æ•°éƒ½æ˜¯0ï¼Œå…¥å‚éƒ½æ˜¯ä¸¤ä¸ªï¼Œæ‰€ä»¥ç›´æ¥è¿”å›0

å®ç°Scalarç±»è¿ç®—ç¬¦é‡è½½

ç®€å•

2ã€å…¶æ¬¡æˆ‘ä»¬éœ€è¦å®ç°æ‹“æ‰‘æ’åºå‡½æ•°ï¼Œè¿™ä¸ªå°±æ˜¯leetcodeåŸé¢˜ï¼Œåœ¨æ­¤ä¹‹å‰æˆ‘ä»¬éœ€è¦ç ”ç©¶ä¸‹Variableç±»



ã€ã€ã€
def topological_sort(variable: Variable) -> Iterable[Variable]:
    sort = []
    visited = set()

    def dfs(v: Variable):
        # if the node is already visited or constant, return
        if v.unique_id in visited or v.is_constant():
            return

        # otherwise, add the node to the visited set
        visited.add(v.unique_id)

        # recursively visit all the parents of the node
        for parent in v.parents:
            dfs(parent)

        # add the current node at the front of the result order list
        sort.insert(0, v)

    dfs(variable)

    return sort
ã€ã€ã€

3ã€æœ€åå®ç°åå‘ä¼ æ’­backpropagateå‡½æ•°

ã€ã€ã€

def backpropagate(variable: Variable, deriv: Any) -> None:
    
    order = topological_sort(variable)

    # initialize derivatives dictionary
    gradients = {variable.unique_id: deriv}

    for v in order:
        # constant nodes do not contribute to the derivatives
        if v.is_constant():
            continue

        # derivative of the current tensor
        grad = gradients.get(v.unique_id)

        # chain rule to propogate to parents
        if not v.is_leaf():
            for parent, chain_deriv in v.chain_rule(grad):
                if parent.unique_id in gradients:
                    gradients[parent.unique_id] += chain_deriv
                else:
                    gradients[parent.unique_id] = chain_deriv

        # only accumulate derivatives for leaf nodes
        if v.is_leaf():
            v.accumulate_derivative(grad)
            
ã€ã€ã€

### æ€»ç»“ä¸åˆ†æ

æœ¬ç« å®ç°äº†æ ‡é‡çš„è‡ªåŠ¨å¾®åˆ†ï¼Œæ¶‰åŠåˆ°çš„ç±»æœ‰ï¼šScalarHistoryã€Scalarã€ScalarFunctionã€Context

- Scalarï¼šæ ‡é‡ï¼Œå¯ä»¥ç†è§£ä¸ºè·Ÿè¸ªè®°å½•æœ€è¿‘ä¸€æ¬¡å‡½æ•°å¤„ç†ä¿¡æ¯çš„Scalar

- ScalarHistoryï¼šå­˜å‚¨å¯¹å½“å‰Scalaræ“ä½œçš„å†å²ï¼ŒåŒ…å«ScalarFunctionå’ŒContext

- ScalarFunctionï¼šç”¨äºæ“ä½œä¸Šé¢çš„Scalarå˜é‡çš„æ•°å­¦å‡½æ•°ï¼Œç”¨äºè¢«ä¸åŒæ•°å­¦å‡½æ•°ç»§æ‰¿

- Contextï¼šå­˜å‚¨å‰å‘ä¼ æ’­éœ€è¦è®°å½•çš„å€¼ï¼Œå¦‚Task1.2ä¸­æåˆ°

æ¥ä¸‹æ¥ä»¥æ­£å‘æ„å»ºè®¡ç®—å›¾ï¼Œå­˜å‚¨å‡½æ•°ä¿¡æ¯å’Œåå‘ä¼ æ’­ï¼Œè®¡ç®—è‡ªåŠ¨å¾®åˆ†ä¸¤ä¸ªæµç¨‹ï¼Œä¸²è”èµ·è‡ªåŠ¨å¾®åˆ†çš„å®ç°ç»†èŠ‚ï¼šä»¥è¡¨è¾¾å¼ğ‘’=(ğ‘+ğ‘)âˆ—(ğ‘+1)ä¸ºä¾‹ï¼Œè‡ªåŠ¨å¾®åˆ†ç†è®ºå‚è§ï¼šhttps://fancyerii.github.io/books/autodiff/

#### æ­£å‘æ„å»ºè®¡ç®—å›¾

+ è®¡ç®—a + b

+ æ‰§è¡Œa.__add__(b)

+ a.__add__(b) å†…éƒ¨è°ƒç”¨ Add.apply(a, b)

+ æ‰§è¡Œaddçš„å‰å‘ä¼ æ’­å‡½æ•°ï¼Œå­˜å‚¨æ‰§è¡Œå†å²ä¿¡æ¯ï¼Œæ–°å»ºScalarHistoryï¼Œè¿”å›æ‰§è¡Œçš„ç»“æœScalarï¼ˆè§ä¸‹é¢applyå‡½æ•°ï¼‰

+ a + bæ‰§è¡Œçš„è¿”å›ç»“æœå’Œb + 1çš„è¿”å›ç»“æœç»§ç»­æ‰§è¡Œä¹˜æ³•ï¼Œæµç¨‹å’Œä¸Šé¢ä¸€æ ·ï¼Œè¿™æ ·è®¡ç®—å›¾è¢«æ„å»ºå‡ºæ¥


ã€ã€ã€
    def apply(cls, *vals: "ScalarLike") -> Scalar:
        raw_vals = []
        scalars = []
        for v in vals:
            if isinstance(v, minitorch.scalar.Scalar):
                scalars.append(v)
                raw_vals.append(v.data)
            else:
                scalars.append(minitorch.scalar.Scalar(v))
                raw_vals.append(v)

        # Create the context.
        ctx = Context(False)

        # Call forward with the variables.
        c = cls._forward(ctx, *raw_vals)
        assert isinstance(c, float), "Expected return type float got %s" % (type(c))

        # Create a new variable from the result with a new history.
        back = minitorch.scalar.ScalarHistory(cls, ctx, scalars)
        return minitorch.scalar.Scalar(c, back)
ã€ã€ã€


#### åå‘ä¼ æ’­å®ç°è‡ªåŠ¨å¾®åˆ†

åå‘ä¼ æ’­å»ºç«‹åœ¨ä¸Šé¢æ„å»ºå¥½çš„è®¡ç®—å›¾ä¸Šï¼Œä»æœ€å³èŠ‚ç‚¹eå¼€å§‹ï¼Œå‚è§ä¸Šé¢çš„backpropagateå‡½æ•°å®ç°

## module2

è¿™ä¸€ä¸ªå®ç°ä¸»è¦æ˜¯å®ç°Tensorï¼Œå³å¼ é‡ï¼Œå‰ä¸¤ç« å®ç°çš„æ˜¯æ ‡é‡Scalarã€‚ä½†æ˜¯æ·±åº¦å­¦ä¹ å½’æ ¹ç»“åº•æ˜¯å»ºç«‹åœ¨å¼ é‡ä¹‹ä¸Šã€‚

### Tasks 2.1: Tensor Data - Indexing

å¼ é‡çš„åº•å±‚å®é™…ä¸Šå°±æ˜¯ä¸€ç»´æ•°ç»„åŠ å„ä¸ªç»´åº¦çš„ä¿¡æ¯ï¼Œé¦–å…ˆåœ¨æ­¤è§£é‡Šä¸‹ï¼š

Storage: åº•å±‚å­˜å‚¨å¼ é‡æ•°æ®çš„ä¸€ç»´æ•°ç»„
OutIndex: è¾“å‡ºä¸‹æ ‡
Index: ä¸‹æ ‡
Shape: å¼ é‡ç»´åº¦ä¿¡æ¯
Strides: æ¯ä¸€ä¸ªç»´åº¦çš„æ­¥é•¿

è¯¥ä»»åŠ¡æ˜¯å®ç°ä¸¤ä¸ªå‡½æ•°ï¼šindex_to_positionï¼Œå³æŠŠä¸‹æ ‡è½¬æ¢ä¸ºåº•å±‚å­˜å‚¨çš„ä¸€ç»´åæ ‡

ã€ã€ã€
def index_to_position(index: Index, strides: Strides) -> int:
    position = 0
    for ind, stride in zip(index, strides):
        position += ind * stride
    return position
ã€ã€ã€

to_indexï¼ŒæŠŠåº•å±‚å­˜å‚¨çš„ä¸€ç»´åæ ‡è½¬æ¢ä¸ºä¸‹æ ‡

ã€ã€ã€
def to_index(ordinal: int, shape: Shape, out_index: OutIndex) -> None:
    cur_ord = ordinal + 0
    for i in range(len(shape) - 1, -1, -1):
        sh = shape[i]
        out_index[i] = int(cur_ord % sh)
        cur_ord = cur_ord // sh
ã€ã€ã€

### Tasks 2.2: Tensor Broadcasting

é¦–å…ˆç†è§£ä¸‹å¼ é‡çš„å¹¿æ’­æœºåˆ¶ï¼š

å½“ä¸€å¯¹å¼ é‡æ»¡è¶³ä¸‹é¢çš„æ¡ä»¶æ—¶ï¼Œå®ƒä»¬æ‰æ˜¯å¯ä»¥è¢«â€œå¹¿æ’­â€çš„ã€‚

1ã€æ¯ä¸ªå¼ é‡è‡³å°‘æœ‰ä¸€ä¸ªç»´åº¦ã€‚
2ã€è¿­ä»£ç»´åº¦å°ºå¯¸æ—¶ï¼Œä»å°¾éƒ¨ï¼ˆä¹Ÿå°±æ˜¯ä»åå¾€å‰ï¼‰å¼€å§‹ï¼Œä¾æ¬¡æ¯ä¸ªç»´åº¦çš„å°ºå¯¸å¿…é¡»æ»¡è¶³ä»¥ä¸‹ä¹‹ä¸€ï¼š
    aã€ç›¸ç­‰ã€‚
    bã€å…¶ä¸­ä¸€ä¸ªå¼ é‡çš„ç»´åº¦å°ºå¯¸ä¸º1ã€‚
    cã€å…¶ä¸­ä¸€ä¸ªå¼ é‡ä¸å­˜åœ¨è¿™ä¸ªç»´åº¦ã€‚

è¯¥ä»»åŠ¡æ˜¯å®ç°ä¸¤ä¸ªå‡½æ•°ï¼šbroadcast_indexï¼Œå°†å¯ä»¥å¹¿æ’­çš„ä¸€å¯¹tensorï¼Œindexé•¿åº¦è¾ƒå¤§tensorçš„indexè½¬æ¢ä¸ºè¾ƒå°é‚£ä¸ªtensorçš„index

ã€ã€ã€
def broadcast_index(
    big_index: Index, big_shape: Shape, shape: Shape, out_index: OutIndex
) -> None:
    for i, s in enumerate(shape):
        if s > 1:
            out_index[i] = big_index[i + (len(big_shape) - len(shape))]
        else:
            out_index[i] = 0
ã€ã€ã€

shape_broadcastï¼Œè¿”å›ä¸€å¯¹tensorå¹¿æ’­åçš„tensorçš„shape

ã€ã€ã€
def shape_broadcast(shape1: UserShape, shape2: UserShape) -> UserShape:
    a, b = shape1, shape2
    m = max(len(a), len(b))
    c_rev = [0] * m
    a_rev = list(reversed(a))
    b_rev = list(reversed(b))
    for i in range(m):
        if i >= len(a_rev):
            c_rev[i] = b_rev[i]
        elif i >= len(b_rev):
            c_rev[i] = a_rev[i]
        else:
            c_rev[i] = max(a_rev[i], b_rev[i])
            if c_rev[i] != a_rev[i] and a_rev[i] != 1:
                raise IndexingError(f"Broadcast failure {a} {b}")
            if c_rev[i] != b_rev[i] and b_rev[i] != 1:
                raise IndexingError(f"Broadcast failure {a} {b}")

    return tuple(reversed(c_rev))
ã€ã€ã€

### Tasks 2.3: Tensor Operations

è¯¥taskæ˜¯å®Œæˆtensorçš„æ‰€æœ‰å‡½æ•°ï¼Œå’Œæ ‡é‡ç±»ä¼¼ï¼Œåªæ˜¯å‡½æ•°ä½œç”¨åœ¨tensorçš„æ¯ä¸ªå…ƒç´ ä¸Šã€‚æœ€åº•å±‚æœ€åŸºæœ¬çš„ä¸‰ä¸ªå‡½æ•°æ˜¯mapã€zipå’Œreduce

ã€ã€ã€
def tensor_map(fn: Callable[[float], float]) -> Any:

    def _map(
        out: Storage,
        out_shape: Shape,
        out_strides: Strides,
        in_storage: Storage,
        in_shape: Shape,
        in_strides: Strides,
    ) -> None:
        out_index: Index = np.zeros(MAX_DIMS, np.int16)
        in_index: Index = np.zeros(MAX_DIMS, np.int16)
        for i in range(len(out)):
            # å¾—åˆ°å¯¹åº”çš„out_index
            to_index(i, out_shape, out_index)
            # é€šè¿‡å¹¿æ’­æœºåˆ¶ç¡®å®šin_index
            broadcast_index(out_index, out_shape, in_shape, in_index)
            # é€šè¿‡indexç¡®å®šåœ¨å­˜å‚¨çš„ä¸‹æ ‡
            o = index_to_position(out_index, out_strides)
            j = index_to_position(in_index, in_strides)
            out[o] = fn(in_storage[j])

    return _map
ã€ã€ã€

ã€ã€ã€
def tensor_zip(fn: Callable[[float, float], float]) -> Any:
    def _zip(
        out: Storage,
        out_shape: Shape,
        out_strides: Strides,
        a_storage: Storage,
        a_shape: Shape,
        a_strides: Strides,
        b_storage: Storage,
        b_shape: Shape,
        b_strides: Strides,
    ) -> None:
        out_index: Index = np.zeros(MAX_DIMS, np.int16)
        a_index: Index = np.zeros(MAX_DIMS, np.int16)
        b_index: Index = np.zeros(MAX_DIMS, np.int16)
        for i in range(len(out)):
            # å¾—åˆ°å¯¹åº”çš„out_index
            to_index(i, out_shape, out_index)
            # é€šè¿‡å¹¿æ’­æœºåˆ¶ç¡®å®ša_indexå’Œb_index
            broadcast_index(out_index, out_shape, a_shape, a_index)
            broadcast_index(out_index, out_shape, b_shape, b_index)
            # é€šè¿‡indexç¡®å®šåœ¨å­˜å‚¨çš„ä¸‹æ ‡
            o = index_to_position(out_index, out_strides)
            a = index_to_position(a_index, a_strides)
            b = index_to_position(b_index, b_strides)
            out[o] = fn(a_storage[a], b_storage[b])

    return _zip
ã€ã€ã€


ã€ã€ã€
def tensor_reduce(fn: Callable[[float, float], float]) -> Any:
    def _reduce(
        out: Storage,
        out_shape: Shape,
        out_strides: Strides,
        a_storage: Storage,
        a_shape: Shape,
        a_strides: Strides,
        reduce_dim: int,
    ) -> None:
        out_index: Index = np.zeros(MAX_DIMS, np.int16)
        reduce_size = a_shape[reduce_dim]
        for i in range(len(out)):
            # å¾—åˆ°å¯¹åº”çš„out_index
            to_index(i, out_shape, out_index)
            # é€šè¿‡indexç¡®å®šåœ¨å­˜å‚¨çš„ä¸‹æ ‡
            o = index_to_position(out_index, out_strides)
            for s in range(reduce_size):
                out_index[reduce_dim] = s
                j = index_to_position(out_index, a_strides)
                out[o] = fn(out[o], a_storage[j])

    return _reduce
ã€ã€ã€

ä½¿ç”¨è¿™ä¸‰ä¸ªæœ€åŸºæœ¬çš„å‡½æ•°å®ç°å¼ é‡çš„å…¶ä»–å‡½æ•°æ“ä½œ

å’Œæ ‡é‡å®ç°ç±»ä¼¼

ã€ã€ã€
class Add(Function):
    @staticmethod
    def forward(ctx: Context, t1: Tensor, t2: Tensor) -> Tensor:
        return t1.f.add_zip(t1, t2)

    @staticmethod
    def backward(ctx: Context, grad_output: Tensor) -> Tuple[Tensor, Tensor]:
        return grad_output, grad_output
ã€ã€ã€

ã€ã€ã€
class Sigmoid(Function):
    @staticmethod
    def forward(ctx: Context, t1: Tensor) -> Tensor:
        out = t1.f.sigmoid_map(t1)
        ctx.save_for_backward(out)
        return out

    @staticmethod
    def backward(ctx: Context, grad_output: Tensor) -> Tensor:
        sigma: Tensor = ctx.saved_values[0]
        return sigma * (-sigma + 1.0) * grad_output
ã€ã€ã€

å®ç°tensorç±»å¯¹è¿ç®—ç¬¦çš„é‡è½½

ç±»ä¼¼

### Tasks 2.4: Gradients and Autograd

2.3è´´çš„ä»£ç å·²ç»å®ç°äº†å¼ é‡çš„åå‘ä¼ æ’­

### æ€»ç»“

æœ¬ç« å®ç°äº†æ ‡é‡çš„è‡ªåŠ¨å¾®åˆ†ï¼Œæ¶‰åŠåˆ°çš„ç±»æœ‰ï¼š

TensoråŒScalarä¸€æ ·ï¼Œå®ç°äº†Variableæ¥å£ï¼Œæ‰€ä»¥åå‘ä¼ æ’­çš„ä»£ç å’ŒScalarä¸€æ ·ã€‚










